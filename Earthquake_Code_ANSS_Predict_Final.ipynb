{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to Predict Earthquakes in California\n",
    "\n",
    "This code was used to produce the results in section 4.3. The steps are as follows:\n",
    "\n",
    "1. Create a grid of equally spaced (10km) points across the region of interest (California)\n",
    "\n",
    "2. For each point investigate a set of final magnitudes and end dates\n",
    "\n",
    "    (a) Magnitudes: 6.50, 6.75, 7.00 and 7.25, corresponding to radii: 138 km, 170 km, 209 km and 257 km according to the formula of Jaume and Sykes [#Jaume:1999]\n",
    "\n",
    "    (b) Dates: end of each year from 1995 to 2011\n",
    "\n",
    "3. Collect the data for all earthquakes within the radius and before the end date\n",
    "\n",
    "4. Fit a straight-line and a power-law to the cumulative Benioff strain from the data, using the selected magnitude to calculate the final cumulative Benioff strain and finding the other parameters (including t_{f}, from the fit)\n",
    "\n",
    "5. Mark the grid point as “positive” if \n",
    "\n",
    "    (a) t_{f} is greater than the date of the last data point but less than the date of the last data point plus 5 years\n",
    "\n",
    "    (b) The variance from a power-law is less than half the variance from a straight-line fit i.e. the curvature parameter is <0.5\n",
    "\n",
    "    (c) There are at least 10 data points\n",
    "\n",
    "6. Create clusters of 10 or more positive points if the distance between them are always less than 100 km. I carried out this clustering using agglomerative hierarchical clustering in Python\n",
    "\n",
    "7. Find the average t_{f} for each cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import radians, cos, sin, asin, sqrt, pi\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "import shapely.geometry\n",
    "import pyproj\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist\n",
    "import math\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import scipy.cluster.hierarchy as sch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the data - ANSS earthquake catalogue\n",
    "data_path = \"Data/ANSS_catalogue.csv\"",
    "df = pd.read_csv(data_path, encoding = 'latin')\n",
    "print(df.shape)\n",
    "\n",
    "#Convert the date to datetime and check the date range\n",
    "df.Date = pd.to_datetime(df.Date, format = '%d/%m/%Y')\n",
    "print(df.Date.min())\n",
    "print(df.Date.max())\n",
    "\n",
    "#Add columns for the year, years since 1900 and days since 1900 (rounded to the nearest year)\n",
    "df['year'] = df['Date'].dt.year\n",
    "df['year_1900'] = df['year'] - 1900\n",
    "df['day_1900'] = df['year_1900']*365\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort the dataset by magnitude\n",
    "df.sort_values(['Magnitude'], ascending = False, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Remove aftershocks (rough method)\n",
    "If more than one earthquake occurs on the same day\n",
    "in the same approximate location keep only the largest earthquake\n",
    "'''\n",
    "df['lat_round'] = round(df.Latitude,0)\n",
    "df['long_round'] = round(df.Longitude,0)\n",
    "df.drop_duplicates(['Date','lat_round','long_round'],inplace = True)\n",
    "df.drop(['lat_round','long_round'], axis = 1, inplace = True)\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the energy release in Nm\n",
    "df['Energy_log10'] = 4.8 + 1.5*df['Magnitude']\n",
    "df['Energy'] = 10 ** df.Energy_log10\n",
    "df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the critical radius (from Jaume and Sykes 1999)\n",
    "df['R_log10'] = -0.2 + 0.36*df['Magnitude']\n",
    "df['R_km'] = 10 ** df.R_log10\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop some columns\n",
    "df.drop(['Time','Depth','NbStations','Gap','Distance','RMS','Source','EventID'], axis = 1, inplace = True)\n",
    "\n",
    "#Convert latlong from degrees to radians\n",
    "df['lat_rad'] = (df.Latitude*pi)/180\n",
    "df['lon_rad'] = (df.Longitude*pi)/180"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A function to calculate the great circle distance between two points \n",
    "on the earth (specified in radians)\n",
    "This function was taken from the internet\n",
    "\"\"\"\n",
    "def haversine(target_lon, target_lat, all_lon, all_lat): \n",
    "    dlon = all_lon - target_lon \n",
    "    dlat = all_lat - target_lat \n",
    "    # haversine formula\n",
    "    a = np.sin(dlat/2)**2 + np.cos(target_lat) * np.cos(all_lat) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a)) \n",
    "    r = 6371 # Radius of earth in km. Use 3956 for miles\n",
    "    return c * r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A Function to return all earthquakes within a radius of a target point\n",
    "and with magnitude greater than M_min\n",
    "\"\"\"\n",
    "def get_points(target_point, all_points, radius = 100, M_min = 4):\n",
    "        \n",
    "    #Filter to only earthquakes over the set magnitude\n",
    "    all_points = all_points[all_points.Magnitude >= M_min]   \n",
    "    \n",
    "    #Filter to only records before the date of the target point (and the target point itself)\n",
    "    all_points = all_points[all_points.Date <= target_point.Date]\n",
    "    #print(\"Number of earlier earthquakes: %d\"  % all_points.shape[0])\n",
    "    \n",
    "    #Calculate the distance between the target point and all other points\n",
    "    all_points['distance'] = haversine(target_point.lon_rad,target_point.lat_rad,\n",
    "                                       all_points.lon_rad, all_points.lat_rad)\n",
    "    \n",
    "    #Filter to only points within the radius\n",
    "    all_points = all_points[all_points['distance'] <= radius]\n",
    "    #print(\"Number of earlier earthquakes within %d km: %d\"  % (radius, all_points.shape[0]))\n",
    "    \n",
    "    #Calculate the time delta in days\n",
    "    all_points['Time_Delta'] = (target_point.Date - all_points.Date).dt.days\n",
    "    \n",
    "    #Return the filtered dataset of earthquakes\n",
    "    return(all_points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function to order a table of earthquakes by date \n",
    "and calculate their cummulative Benioff strain\n",
    "\"\"\"\n",
    "def benioff_calc(all_points):\n",
    "    #Sort the earthquakes by date\n",
    "    all_points.sort_values('Date', inplace = True)\n",
    "    \n",
    "    #Calculate the cummulative Benioff strain\n",
    "    all_points['Energy_sqrt'] = np.sqrt(all_points.Energy)\n",
    "    all_points['Cummulative_Benioff_Strain'] = all_points['Energy_sqrt'].cumsum()\n",
    "    \n",
    "    #Return the original dataset with a new column for the cummulative Benioff strain\n",
    "    return(all_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function to calculate the rmse for predictions\n",
    "\"\"\"\n",
    "def rmse_calc(pred, all_points):\n",
    "    #Calculate rmse\n",
    "    err = all_points.Cummulative_Benioff_Strain - pred\n",
    "    err_squared = err**2\n",
    "    mean_err = np.mean(err_squared)\n",
    "    rmse = np.sqrt(mean_err)\n",
    "    #Return the rmse\n",
    "    return(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Fitting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function to fit a straight-line to data\n",
    "\"\"\"\n",
    "def straight_line_fit(all_points):\n",
    "    #Define the straight line function and parameters\n",
    "    def straightLine(t, A, B):\n",
    "        return A + B*t\n",
    "    \n",
    "    #Fit the data\n",
    "    popt, pcov = curve_fit(straightLine,\n",
    "                           all_points.day_1900, #t\n",
    "                           all_points.Cummulative_Benioff_Strain\n",
    "                          )\n",
    "    #Make the predictions and calculate the rmse\n",
    "    pred = popt[0] + popt[1]*all_points.day_1900\n",
    "    rmse = rmse_calc(pred, all_points)\n",
    "    \n",
    "    return(popt, rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function to fit a power-law to data\n",
    "\"\"\"\n",
    "def power_law_fit(all_points, final_benioff):\n",
    "    #Define the power law function and the parameters\n",
    "    def power_law(t, B, tf, m):\n",
    "        return B*(tf - t)**m\n",
    "\n",
    "    #Set up a starting point for the fitting parameters\n",
    "    starting_point = [-1000000, 40000, 0.3]\n",
    "    #Fit the data\n",
    "    popt, pcov = curve_fit(power_law,\n",
    "                           all_points.day_1900, #t\n",
    "                           all_points.Cummulative_Benioff_Strain - final_benioff,\n",
    "                           p0 = starting_point,\n",
    "                           maxfev = 10000\n",
    "                          )\n",
    "    #Make the predictions and calculate the rmse \n",
    "    pred = final_benioff + popt[0]*(popt[1] - all_points.day_1900)**popt[2]\n",
    "    rmse = rmse_calc(pred, all_points)\n",
    "    \n",
    "    return(popt, rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting Master Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function to fit the data using the different methods (straight-line and power-law)\n",
    "Returns the dataset used for the fit, the curvature parameter \n",
    "and the power law fitting parameters\n",
    "\"\"\"\n",
    "def fitting_function(target_point, df, r):\n",
    "    #Get the relevant earthquakes within the radius\n",
    "    sub_points = get_points(target_point, df, radius = r,M_min = target_point.Magnitude - 2)\n",
    "    \n",
    "    #Check to see if there are enough points to attempt to fit the data\n",
    "    if(sub_points.shape[0] < 10):\n",
    "        return\n",
    "\n",
    "    #Calculate the cummulative benioff strain\n",
    "    sub_points = benioff_calc(sub_points)\n",
    "\n",
    "    #Set the index\n",
    "    sub_points.index = sub_points.day_1900\n",
    "\n",
    "    #Fit to a straight line\n",
    "    sl_opt, sl_rmse = straight_line_fit(sub_points)\n",
    "\n",
    "    #Try to fit to a power law\n",
    "    final_benioff = max(sub_points.Cummulative_Benioff_Strain) + target_point.Energy_sqrt\n",
    "    try:\n",
    "        pl_opt, pl_rmse = power_law_fit(sub_points, final_benioff)\n",
    "    except:\n",
    "        return\n",
    "\n",
    "    #Calculate C\n",
    "    c = pl_rmse/sl_rmse\n",
    "                \n",
    "    return(sub_points, c, pl_opt)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create an grid of points with 10km spacings across California\n",
    "\"\"\"\n",
    "#Projection setup\n",
    "p_ll = pyproj.Proj('epsg:4326') #latlong\n",
    "p_mt = pyproj.Proj('epsg:3310') #projected metric - for California\n",
    "\n",
    "#Set the corners of the area to be split into a grid\n",
    "#Set to slightly larger than the area covered by the earthquakes used in Bowman 1998\n",
    "nw = shapely.geometry.Point((-123, 38))\n",
    "se = shapely.geometry.Point((-114, 32))\n",
    "\n",
    "stepsize = 10000 # 10 km grid step size\n",
    "\n",
    "# Project corners to target projection\n",
    "s = pyproj.transform(p_ll, p_mt, nw.y, nw.x)\n",
    "e = pyproj.transform(p_ll, p_mt, se.y, se.x)\n",
    "print(s)\n",
    "print(e)\n",
    "\n",
    "#Loop through all grid points to find the latlong values\n",
    "grid_points_lat = []\n",
    "grid_points_long = []\n",
    "x = s[0]\n",
    "while x < e[0]:\n",
    "    y = s[1]\n",
    "    while y > e[1]:\n",
    "        #Create a geometry point with the required projection\n",
    "        p = shapely.geometry.Point(pyproj.transform(p_mt, p_ll, x, y))\n",
    "        grid_points_lat.append(p.x)\n",
    "        grid_points_long.append(p.y)\n",
    "        y -= stepsize\n",
    "    x += stepsize    \n",
    "\n",
    "#Setup the grid as a dataframe\n",
    "grid = pd.DataFrame({'grid_id': range(0,len(grid_points_long)),'lat': grid_points_lat,'long': grid_points_long})\n",
    "grid.to_csv('California_grid.csv', index = False)    \n",
    "\n",
    "print(grid.shape)   \n",
    "grid.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the Data for Each Grid Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Set up the magnitudes, radii and final dates to loop through\n",
    "Note that the radii corespond to the magnitudes (using the formula of Jaume and Sykes)\n",
    "\"\"\"\n",
    "magnitudes = [6.50, 6.75, 7.00, 7.25]\n",
    "dates = pd.date_range(start='1/1/1995', end = '1/1/2012', freq='Y')\n",
    "radii = [138, 170, 209, 257]\n",
    "print(len(magnitudes)*len(dates)*grid.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Loop through each grid point, critical region radius/final magnitude and end date\n",
    "Collect the data for all earthquakes within the radius and before the end date\n",
    "Fit a straight-line and a power-law on all parameters\n",
    "'''\n",
    "#Set up a dataframe to store the results\n",
    "grid_results = pd.DataFrame(columns = ['Date', 'Lat','Long','Magnitude',\n",
    "                                       'R', 'C', 'tf','B','m','data_points'])\n",
    "for i in range(0, grid.shape[0]):\n",
    "    print(i)\n",
    "    lat = grid.lat.iloc[i]\n",
    "    long = grid.long.iloc[i]\n",
    "    \n",
    "    for mag in magnitudes:\n",
    "        for date in dates:\n",
    "            r = 10**(-0.2 + 0.36*mag)\n",
    "            #Setup a target point \n",
    "            target_point = pd.Series({'Date': date,\n",
    "                                      'Latitude': lat,\n",
    "                                      'Longitude': long,\n",
    "                                     'lat_rad':lat*pi/180,\n",
    "                                     'lon_rad': long*pi/180,\n",
    "                                      'Magnitude': mag,\n",
    "                                      'Energy_sqrt': np.sqrt(10 ** (4.8 + 1.5*mag))\n",
    "                                     })\n",
    "            \n",
    "            #Fit the straight-line and the power-law\n",
    "            result = fitting_function(target_point, df, r)\n",
    "            #If it was not possible to fit a power-law -> move to the next grid point\n",
    "            if(result == None):\n",
    "                continue\n",
    "            sub_points, c, pl_opt = result\n",
    "            \n",
    "            #Append the results to the dataframe\n",
    "            fit_result = {'Date': date,\n",
    "                          'Lat': lat,\n",
    "                          'Long': long,\n",
    "                          'Magnitude': mag,\n",
    "                          'R': r,\n",
    "                          'C': c,\n",
    "                          'tf': pl_opt[1],\n",
    "                          'B': pl_opt[0],\n",
    "                          'm': pl_opt[2],\n",
    "                          'data_points': sub_points.shape[0],\n",
    "                         'final_date': max(sub_points.day_1900)}\n",
    "            grid_results = grid_results.append(fit_result, ignore_index=True)\n",
    "            \n",
    "    #Save out after every grid point - necessary as the code takes a long time to run\n",
    "    grid_results.to_csv('California_grid_analysed.csv', index = False)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Housekeeping - required as I saved out multiple files from the previous step\n",
    "\"\"\"\n",
    "#Read in all files and combine\n",
    "mypath = os.getcwdb()\n",
    "files = [f for f in os.listdir(mypath) if os.path.isfile(os.path.join(mypath, f))]\n",
    "files = [f.decode('utf-8') for f in files]\n",
    "files = [f for f in files if \"California_grid_analysed\" in f]\n",
    "\n",
    "all_files = []\n",
    "for f in files:\n",
    "    all_files.append(pd.read_csv(f))\n",
    "    \n",
    "grid_results = pd.concat(all_files)\n",
    "print(grid_results.shape)\n",
    "\n",
    "#Remove duplicate values\n",
    "grid_results['Check'] = grid_results.Date.map(str) + grid_results.Lat.map(str) + \\\n",
    "                        grid_results.Long.map(str) + grid_results.Magnitude.map(str)\n",
    "grid_results = grid_results.drop_duplicates(['Check'])\n",
    "print(grid_results.shape)\n",
    "grid_results.drop(['Check'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Mark each row as “positive” if:\n",
    "    (a) t_f is > the date of the last data point but < the date of the last data point plus 5 years\n",
    "    (b) The variance from the power law is less than half the variance from a straight line fit\n",
    "    (c) There are at least 10 data points\n",
    "\"\"\"\n",
    "grid_results['Date_Check'] = (grid_results.tf > grid_results.final_date) & \\\n",
    "                                (grid_results.tf < grid_results.final_date + 5*365)\n",
    "print(grid_results.Date_Check.value_counts())\n",
    "grid_results['C_Check'] = (grid_results.C < 0.5)\n",
    "print(grid_results.C_Check.value_counts())\n",
    "grid_results['Points_Check'] = (grid_results.data_points >=10)\n",
    "print(grid_results.Points_Check.value_counts())\n",
    "grid_results['Positive'] = grid_results.Date_Check & grid_results.C_Check & grid_results.Points_Check\n",
    "print(grid_results.Positive.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter to just positive grid points\n",
    "positive_results = grid_results[grid_results.Positive].copy()\n",
    "positive_results.to_csv('Positive_California_Grid_Points.csv', index = False)\n",
    "grid_results.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create clusters of 10 or more positive points \n",
    "if the distance between them are always less than 100km\n",
    "use agglomerative hierarchical clustering\n",
    "\"\"\"\n",
    "#Convert the location to radians\n",
    "positive_results['lat_rad'] = (positive_results.Lat*pi)/180\n",
    "positive_results['lon_rad'] = (positive_results.Long*pi)/180\n",
    "\n",
    "#Loop through each year and radius/magnitude\n",
    "for year in positive_results.Date.unique():\n",
    "    for r in positive_results.R.unique():\n",
    "        print(\"Year: %s, R: %.2f\" % (year, r))\n",
    "\n",
    "        #Filter to only results from the year and radius/magnitude\n",
    "        temp_df = positive_results[(positive_results.Date == year) &\\\n",
    "                                   (positive_results.R == r)].copy()\n",
    "        \n",
    "        #If there are < 5 positive grid points move to the next item in the loop\n",
    "        if(temp_df.shape[0] < 5):\n",
    "            continue\n",
    "           \n",
    "        #Compute the distances between all points\n",
    "        X = temp_df.iloc[:,[1,2]].values\n",
    "        distances = cdist(X,X, lambda ori,des: int(round(haversine(ori[1]*pi/180, ori[0]*pi/180,\n",
    "                                                           des[1]*pi/180, des[0]*pi/180))))\n",
    "        \n",
    "        #Perform hierarchical aglomerative clustering on the distances\n",
    "        #Using a distance threshold of 100km\n",
    "        model = AgglomerativeClustering(n_clusters=None, affinity='precomputed',\n",
    "                                linkage='single', distance_threshold = 100)\n",
    "        model.fit(distances)\n",
    "        labels = model.labels_\n",
    "        print(model.n_clusters_)\n",
    "        \n",
    "        #Add the labels back into the dataframe\n",
    "        temp_df['Cluster'] = labels\n",
    "        temp_df.head()\n",
    "        \n",
    "        #For each cluster - calculate the tf (mean, standard deviation and count)\n",
    "        #and the lat and long means i.e. the cluster centre\n",
    "        cluster_details = temp_df.groupby(['Cluster'], as_index = False).\\\n",
    "                            agg({'tf':['mean', 'std', 'count'], \n",
    "                                 'Lat':'mean',\n",
    "                                 'Long':'mean'})\n",
    "        cluster_details.columns = ['Cluster','Cluster_tf_mean',\n",
    "                                   'Cluster_tf_stdev', 'Cluster_Count',\n",
    "                                   'lat_mean', 'long_mean']\n",
    "        #Merge with the main dataset\n",
    "        temp_df = temp_df.merge(cluster_details, how = 'left', on = 'Cluster')\n",
    "        \n",
    "        #Set clusters with <10 points to None\n",
    "        temp_df.Cluster = np.where(temp_df.Cluster_Count < 10, None, temp_df.Cluster)\n",
    "        \n",
    "        #Convert the mean tf to a datetime\n",
    "        temp_df['year'] = temp_df['Cluster_tf_mean'].apply(lambda x: str(x/365 + 1900).split('.')[0])\n",
    "        temp_df['day'] = temp_df['Cluster_tf_mean'].apply(lambda x: '0.' + str(x/365 + 1900).split('.')[1])\n",
    "        temp_df['day'] = temp_df['day'].apply(lambda x: int(float(x)*365) + 1)\n",
    "        temp_df['date_tf'] = temp_df['day'].astype(str) + '/' + temp_df['year'].astype(str)\n",
    "        temp_df['date_tf'] = pd.to_datetime(temp_df['date_tf'], format = '%j/%Y')\n",
    "        \n",
    "        temp_df.drop(['Date_Check', 'C_Check', 'Points_Check',\n",
    "                      'lat_rad', 'lon_rad', 'year','day'],\n",
    "                     axis = 1, inplace = True)\n",
    "\n",
    "        #Save out\n",
    "        temp_df.to_csv('Images/Clusters/' + year + '_' + str(round(r,2)) + '_Clusters.csv', index = False)\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}